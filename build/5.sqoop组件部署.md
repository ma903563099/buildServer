# sqoop(hive与传统数据库数据传递组件)的搭建
## 注意：下载sqoop版本一定要与hadoop版本对应
![](http://tmp.wyjsjxh.com/201912060848_731.png)

## 开始搭建：
1. 配置环境变量
```
SQOOP_HOME=/usr/local/sqoop1.4.6
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$HBASE_HOME/bin:$SQOOP_HOME/bin
```
用`source`刷新环境变量

2. 修改配置文件  
    2.1 进入sqoop的conf目录，输入 cp sqoop-env-template.sh sqoop-env.sh 复制并重命名配置文件，接着输入 `vi sqoop-env.sh` 修改配置文件
    ```
    export HADOOP_COMMON_HOME=/usr/local/hadoop/hadoop-2.7.3/

    export HADOOP_MAPRED_HOME=/usr/local/hadoop/hadoop-2.7.3/

    export HBASE_HOME=/usr/local/hbase/hbase-1.2.4/

    export HIVE_HOME=/usr/local/hive/apache-hive-0.13.0-bin/   
    ```
    2.2 将mysql的驱动jar包放到sqoop的lib目录下
    ![](http://tmp.wyjsjxh.com/201912060848_899.png)

## 搭建完成
---
(重要)：sqoop学习
```
[hadoop@hadoop3 ~]$ sqoop list-tables \
> --connect jdbc:mysql://hadoop1:3306/mysql \
> --username root \
> --password root
```

创建一张跟mysql中的help_keyword表一样的hive表hk：
```
sqoop create-hive-table \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--hive-table hk
```

## 普通导入：导入mysql库中的help_keyword的数据到HDFS上

导入的路径：/user/hadoop11/my_help_keyword1
```
sqoop import   \
--connect jdbc:mysql://hadoop1:3306/mysql   \
--username root  \
--password root   \
--table help_keyword   \
--target-dir /user/hadoop11/my_help_keyword1  \
--fields-terminated-by '\t'  \
-m 2

```
导入数据：带where条件
```
sqoop import   \
--connect jdbc:mysql://hadoop1:3306/mysql   \
--username root  \
--password root   \
--where "name='STRING' " \
--table help_keyword   \
--target-dir /sqoop/hadoop11/myoutport1  \
-m 1
```
查询指定列
```
sqoop import   \
--connect jdbc:mysql://hadoop1:3306/mysql   \
--username root  \
--password root   \
--columns "name" \
--where "name='STRING' " \
--table help_keyword  \
--target-dir /sqoop/hadoop11/myoutport22  \
-m 1
selct name from help_keyword where name = "string"
```
导入：指定自定义查询SQL
```
sqoop import   \
--connect jdbc:mysql://hadoop1:3306/  \
--username root  \
--password root   \
--target-dir /user/hadoop/myimport33_1  \
--query 'select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = "STRING"' \
--split-by  help_keyword_id \
--fields-terminated-by '\t'  \
-m 4

```

## 把MySQL数据库中的表数据导入到Hive中

Sqoop 导入关系型数据到 hive 的过程是先导入到 hdfs，然后再 load 进入 hive

普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：
```
sqoop import   \
--connect jdbc:mysql://hadoop1:3306/mysql   \
--username root  \
--password root   \
--table help_keyword   \
--hive-import \
-m 1

```
导入过程:
```
第一步：导入mysql.help_keyword的数据到hdfs的默认路径 
第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中 
第三步：把临时目录中的数据导入到hive表中
```

# 把MySQL数据库中的表数据导入到hbase

此时会报错，因为需要先创建Hbase里面的表，再执行导入的语句
```
hbase(main):001:0> create 'new_help_keyword', 'base_info'
0 row(s) in 3.6280 seconds

=> Hbase::Table - new_help_keyword
hbase(main):002:0> 
```
普通导入
```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--hbase-table new_help_keyword \
--column-family person \
--hbase-row-key help_keyword_id

```

### [sqoop操作练习扩展学习](https://blog.csdn.net/lijingshan34/article/details/81316118)