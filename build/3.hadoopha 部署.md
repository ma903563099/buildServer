<<<<<<< HEAD
#hadoop高可用环境部署
=======
# HADOOPHA部署
[HADOOPHA(高可用)介绍](hadoopha了解.md)
> 前提 ** 反复搭建 hadoop 完全分布式 ** 熟练配置文件
# HadoopHA集群搭建
环境说明：

系统：centos7-x 软件：hadoop2.6.0、JDK8
## 集群规划
ip地址|主机名|安装软件|进程
-|-|-|-|
192.168.188.4|master|hadoop、jdk|NameNode
192.168.188.3|slave1|hadoop、jdk|DataNode
192.168.188.5|slave2|hadoop、jdk|DataNode
192.168.188.6|back|hadoop、jdk|DataNode

##  准备工作
在搭建环境之前你需要先配置好Hadoop的完全分布式，并且搭建好Zookeeper集群。

从HA机制我们可以发现搭建高可用的Hadoop集群需要依赖Zookeeper的特性，所以第一步，咱们来完成Zookeeper的安装与集群搭建。

[ZooKeeper搭建](11.zookeeper组件部署.md)

# 高可用(hadoopHA)配置

Hadoop高可用配置
修改配置文件
在完全分布式的基础上搭建高可用主要修改的是两个配置文件：core-site.xml，hdfs-site.xml

首先修改的是hadoop-master机器的 core-site.xml，删除fs.default.name属性，加入如下配置：
```
<configuration>
    <!-- 指定hdfs的nameservice为ns1 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://ns1/</value>
    </property>
    <!-- 指定hadoop临时目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/hadoop/namenode</value>
        </property>
    <!-- 指定zookeeper地址 -->
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>hadoop-back:2181,hadoop-slave2:2181,hadoop-slave3:2181</value>
    </property>
</configuration>
```
接下来编辑hdfs-site.xml文件：
```
<configuration>
<property>  
        <name>dfs.name.dir</name>  
        <value>/usr/hadoop/hdfs/name</value>  
        <description>namenode上存储hdfs名字空间元数据 </description>   
</property>  
<property>  
        <name>dfs.data.dir</name>  
        <value>/usr/hadoop/hdfs/data</value>  
        <description>datanode上数据块的物理存储位置</description>  
</property>
<property>
        <name>dfs.replication</name>
        <value>3</value>
</property>
<property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
</property>
<property>
        <name>dfs.nameservices</name>
        <value>ns1</value>
</property>
<property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
</property>
<property>
        <name>dfs.ha.namenodes.ns1</name>
        <value>nn1,nn2</value>
</property>
<!-- nn1的RPC通信地址，nn1所在地址  -->
<property>
        <name>dfs.namenode.rpc-address.ns1.nn1</name>
        <value>hadoop-master:8020</value>
</property>
<!-- nn1的http通信地址，外部访问地址 -->
<property>
        <name>dfs.namenode.http-address.ns1.nn1</name>
        <value>hadoop-master:9870</value>
</property>
<!-- nn2的RPC通信地址，nn2所在地址 -->
<property>
        <name>dfs.namenode.rpc-address.ns1.nn2</name>
        <value>hadoop-back:8020</value>
</property>
<!-- nn2的http通信地址，外部访问地址 -->
<property>
        <name>dfs.namenode.http-address.ns1.nn2</name>
        <value>hadoop-back:9870</value>
</property>

 <!--namenode1 RPC端口 -->

<!-- 指定NameNode的元数据在JournalNode日志上的存放位置(一般和zookeeper部署在一起) -->
<property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://hadoop-back:8485;hadoop-slave2:8485;hadoop-slave3:8485/ns1</value>
</property>
<!-- 指定JournalNode在本地磁盘存放数据的位置 -->
<property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/usr/hadoop/journal</value>
</property>
<!--客户端通过代理访问namenode，访问文件系统，HDFS 客户端与Active 节点通信的Java 类，使用其确定Active 节点是否活跃  -->
<property>
        <name>dfs.client.failover.proxy.provider.ns1</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<!--这是配置自动切换的方法 -->
<property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
</property>
<!-- 这个是使用sshfence隔离机制时才需要配置ssh免登陆 -->
<property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/usr/hadoop/.ssh/id_rsa</value>
</property>
<!-- 配置sshfence隔离机制超时时间，这个属性同上，如果你是用脚本的方法切换，这个应该是可以不配置的 -->
<property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>30000</value>
</property>
<!-- 这个是开启自动故障转移，如果你没有自动故障转移，这个可以先不配 -->
<property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
</property>    
</configuration>
```
yarn-site.xml
```
		yarn.log-aggregation-enable				true

		yarn.resourcemanager.zk-address			hadoop-back:2181,hadoop-slave2:2181,hadoop-slave3:2181

		yarn.resourcemanager.cluster-id			besttonecluster-yarn

		yarn.resourcemanager.ha.enabled			true

		yarn.resourcemanager.ha.rm-ids			rm1,rm2
		yarn.resourcemanager.hostname.rm1			hadoop-back
		yarn.resourcemanager.hostname.rm2			hadoop-slave2
		
		yarn.resourcemanager.webapp.address.rm1		hadoop-back:8088
		yarn.resourcemanager.webapp.address.rm2		hadoop-slave2:8088

		yarn.resourcemanager.ha.automatic-failover.enabled	true
		yarn.resourcemanager.ha.automatic-failover.embedded	true

		yarn.resourcemanager.ha.automatic-failover.zk-base-path	/yarn-leader-election

		yarn.resourcemanager.recovery.enabled			true

		yarn.nodemanager.aux-services			mapreduce_shuffle
```
配置文件修改完毕，将以上做的修改同步到其他3台主机上。

# 启动集群
## 1.启动Zookeeper集群
在子节点`hadoop-back,hadoop-slave2, hadoop-slave3`节点中切换至hadoop目录并执行如下命令:

./bin/zkServer.sh start

# 2.格式化zk集群
hadoop-master上执行:

/bin/hdfs zkfc -formatZK

# 3.启动journalnode集群
子节点：`hadoop-back,hadoop-slave2, hadoop-slave3`上执行：

 ./sbin/hadoop-daemon.sh  start journalnode
# 4.格式化namenode（如果之前已经格式化可以考虑删除相关文件夹）:
./bin/hdfs namenode -format
# 5.启动datanode
hadoop-back, hadoop-slave2, hadoop-slave3上执行：

 ./sbin/hadoop-daemon.sh start datanode
# 6、启动namenode
### hadoop-master:

 ./sbin/hadoop-daemon.sh start namenode
### hadoop-back:

./bin/hdfs namenode -bootstrapStandby
./sbin/hadoop-daemon.sh start namenode  

此时namenode1(hadoop-master)和namenode2(hadoop-back)同时处于standby状态。  

$hdfs haadmin -getServiceState nn1 (查看hadoop-master的namenode状态)  
$hdfs haadmin -getServiceState nn2(查看hadoop-master2的namenode状态)
# 7、启动zkfc服务
在namenode1(hadoop-master)和namenode2(hadoop-back)上同时执行如下命令：

./sbin/hadoop-daemon.sh  start zkfc
# 8、验证
杀死hadoop-master的namenode节点查看hadoop-back是否能从standby状态切换至active。

有时候需要手动切换状态可以使用：

$ bin/hdfs haadmin -transitionToActive nn1 ##切换成active  
$ bin/hdfs haadmin -transitionToStandby nn1 ##切换成standby



---
# 动态添加节点
## 修改配置文件
### 1. 配置slaves(3.X的Hadoop是workers)文件
添加要增加的机器名称（本例使用hadoop-slave4）
```
hadoop-back
hadoop-slave2
hadoop-slave3
hadoop-slave4
```

2. hosts文件
在集群中每台机器加入新机器的ip与机器名映射。
```
192.168.20.101  hadoop-master
192.168.20.102  hadoop-back
192.168.20.103  hadoop-slave2
192.168.20.104  hadoop-slave3
192.168.20.105  hadoop-slave4
```
启动DataNode
```
hadoop-daemon.sh start datanode
yarn-daemon.sh start nodemanager
```
检查新增节点是否已经Live
通过命令可以查看已经启动的节点：
```
yarn node -list
```
也可通过图形化界面查看:
```
http://ipaddr:50070
```
2.X端口号为50070,3.X端口号为9870.

# 测试创建文件
测试一下创建新的文件时是否能集成新的备份系数
```
hdfs hadoop fs -copyFromLocal mysql-connector-java-5.1.22.tar.gz /test
hdfs hadoop fs -lsr /test
```
# 对HDFS中的文件进行负载均衡
```
hdfs hadoop balancer
```

# 动态删除节点
## 配置文件(分布式原文件基础上增加)
core-site.xml
```
<property>
<name>dfs.hosts.exclude</name>
<value>/usr/hadoop/conf/exclude</value>
</property>
```
hdfs-site.xml
```
<property>
<name>dfs.hosts.exclude</name>
<value>/usr/hadoop/conf/exclude</value>
</property>
```
创建/usr/hadoop/conf/exclude

在exclude添加要删除的节点：

hadoop-slave4
```
删除节点时要注意备份系数，Hadoop默认的数值是3，通常备份系数不需要太高，可以是服务器总量的1/3左右即可，备份系数最好不要与节点数相同。
```

# 刷新配置
```
hadoop dfsadmin -refreshNodes
```
# 检查节点状态
在web站点检查节点的处理状态
>>>>>>> 4e407de8f5380f845bc6b892b389e4eb0a089716
