# hive数据仓库工具的搭建
## 前言  

说明：安装hive前提是要先安装hadoop集群，并且hive只需要再hadoop的namenode节点集群里安装即可(需要在所有namenode上安装[hadoopHA])，可以不在datanode节点的机器上安装。另外还需要说明的是，虽然修改配置文件并不需要你已经把hadoop跑起来，但是本文中用到了hadoop命令，在执行这些命令前你必须确保`hadoop是在正常跑着的`，而且`启动hive的前提也是需要hadoop在正常跑着`，所以建议你先将hadoop跑起来在按照本文操作。有关如何安装和启动hadoop集

1.下载安装Hive
1.1.下载Hive 
下载地址：http://hive.apache.org/downloads.html  

点击图中的***Download a release now!***

![](https://img-blog.csdn.net/20170512220130465?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcHVjYW9fY3Vn/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

1.2.安装hive 2.1.1
```
#解压：
tar -zxvf apache-hive-2.1.1-bin.tar.gz
#把解压后的文件移到目录/usr/local/下:
mv apache-hive-2.1.1-bin /usr/local/apache-hive-2.1.1
#配置hive环境变量
vim /etc/profile
--------------------------------------------------
HIVE_HOME=/usr/local/apache-hive-2.1.1
HIVE_CONF_DIR=$HIVE_HOME/conf
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin
export JAVA_HOME JRE_HOME PATH CLASSPATH HADOOP_HOME  HIVE_HOME HIVE_CONF_DIR
-------------------------------------------------
#使配置文件的修改生效
source ~/.bashrc

```

2.配置hive  
2.1.配置hive-site.xml
```
# 进入目录
cd $HIVE_CONF_DIR
# 拷贝hive-default.xml.template并重命名为hive-site.xml
cp hive-default.xml.template  hive-site.xml
# 编辑hive-site.xml
vim hive-site.xml
```
hive-site.xml
```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>username to use against metastore database</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
    <description>password to use against metastore database</description>
  </property>
</configuration>
```


2.2.将MySQL驱动包上载到Hive的lib目录下  
```
cp /home/dtadmin/spark_cluster/mysql-connector-java-5.1.36.jar $HIVE_HOME/lib/
```
2.2.新建hive-env.sh文件并进行修改
```
cd $HIVE_CONF_DIR
cp hive-env.sh.template hive-env.sh #基于模板创建hive-env.sh
vim hive-env.sh #编辑配置文件并加入以下配置：
-------------------------------------------------
export HADOOP_HOME=/home/hadoop/hadoop-2.7.3
export HIVE_CONF_DIR=/usr/local/apache-hive-2.1.1/conf
export HIVE_AUX_JARS_PATH=/usr/local/apache-hive-2.1.1/lib
--------------------------------------------------
```

# 4.启动和测试
```
 service mysql start #启动mysql服务
 mysql -u root -p  #登陆shell界面
 
```

# 4.1 新建hive数据库。
```
mysql> create database hive;    #这个hive数据库与hive-site.xml中localhost:3306/hive的hive对应，用来保存hive元数据
```
# 5. 配置mysql允许hive接入：
```
mysql> grant all on *.* to hive@localhost identified by 'hive';   #将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码
mysql> flush privileges;  #刷新mysql系统权限关系表
```

# 6. 启动hive
启动hive之前，请先启动hadoop集群。
```
start-all.sh #启动hadoop
hive  #启动hive
```
Shell 命令
注意，我们这里已经配置了PATH，所以，不要把start-all.sh和hive命令的路径加上。如果没有配置PATH，请加上路径才能运行命令，比如，本教程Hadoop安装目录是“/usr/local/hadoop”，Hive的安装目录是“/usr/local/hive”，因此，启动hadoop和hive，也可以使用下面带路径的方式：
```
cd /usr/local/hadoop
./sbin/start-all.sh
cd /usr/local/hive
schematool -dbType mysql -initSchema
./bin/hive
```